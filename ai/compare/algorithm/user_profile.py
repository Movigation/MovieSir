"""
ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ í‰ê°€ ë° ë¹„êµ ìŠ¤í¬ë¦½íŠ¸

ai/training/original_data/ í´ë”ì— rating.csv íŒŒì¼ì´ ìˆì–´ì•¼ í•¨

í‰ê·  ì„ë² ë”© ë°©ì‹ vs ìµœëŒ€ ìœ ì‚¬ë„ ë°©ì‹ ë¹„êµ:
- í‰ê·  ë°©ì‹: ì‚¬ìš©ì ì˜í™” ì„ë² ë”©ì˜ í‰ê· ì„ ì‚¬ìš©ì í”„ë¡œí•„ë¡œ ì‚¬ìš©
- ìµœëŒ€ ìœ ì‚¬ë„ ë°©ì‹: ì‚¬ìš©ì ì˜í™”ë“¤ê³¼ì˜ ìœ ì‚¬ë„ ì¤‘ ìµœëŒ€ê°’ ì‚¬ìš©

í‰ê°€ ì§€í‘œ:
1. Precision@10: ìƒìœ„ 10ê°œ ì¶”ì²œ ì¤‘ ì •ë‹µ ë¹„ìœ¨
2. NDCG@10: ìˆœìœ„ë¥¼ ê³ ë ¤í•œ ì •í™•ë„
3. Diversity: ì¥ë¥´ ë‹¤ì–‘ì„±
4. ì¶”ì²œ ì†Œìš” ì‹œê°„
"""

import os
import sys
import time
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple, Any, Optional
from dotenv import load_dotenv
from sklearn.metrics import ndcg_score
from sklearn.preprocessing import MinMaxScaler
from scipy import stats

# ìƒìœ„ ë””ë ‰í† ë¦¬ ì„í¬íŠ¸ë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€ (ai/ í´ë”)
sys.path.append(str(Path(__file__).parent.parent.parent))

from inference.recommendation_model import HybridRecommender


def set_seed(seed=42):
    """ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •"""
    import random
    random.seed(seed)
    np.random.seed(seed)
    print(f"ğŸ”’ Random seed fixed: {seed}")


class MaxSimilarityRecommender(HybridRecommender):
    """ìµœëŒ€ ìœ ì‚¬ë„ ë°©ì‹ ì¶”ì²œ ì‹œìŠ¤í…œ (í˜„ì¬ ë²„ì „ ìƒì†)

    ê°œë³„ ì˜í™” ì„ë² ë”© ìœ ì§€ â†’ ê° í›„ë³´ ì˜í™”ì™€ ê°œë³„ ìœ ì‚¬ë„ ê³„ì‚° â†’ ìµœëŒ€ê°’
    """

    def __init__(self, db_config: dict = None, als_model_path: str = None,
                 als_data_path: str = None, device: str = None, base_recommender: HybridRecommender = None):
        """
        í˜„ì¬ ë²„ì „ê³¼ ë™ì¼í•˜ì§€ë§Œ ëª…ì‹œì ìœ¼ë¡œ ìµœëŒ€ ìœ ì‚¬ë„ ë°©ì‹ ì‚¬ìš©

        Args:
            base_recommender: ê¸°ì¡´ HybridRecommender ì¸ìŠ¤í„´ìŠ¤ (ë°ì´í„° ì¬ì‚¬ìš©)
            db_config, als_model_path, als_data_path: base_recommender ì—†ì„ ë•Œ ì‚¬ìš©
        """
        if base_recommender is not None:
            # ê¸°ì¡´ ì¸ìŠ¤í„´ìŠ¤ì˜ ë°ì´í„° ì¬ì‚¬ìš©
            print("  â†’ Reusing data from existing HybridRecommender instance")
            self._copy_from_base(base_recommender)
        else:
            # ìƒˆë¡œ ì´ˆê¸°í™”
            super().__init__(db_config, als_model_path, als_data_path, device)

        print("  â†’ Using MAXIMUM SIMILARITY method")

    def _copy_from_base(self, base: HybridRecommender):
        """ê¸°ì¡´ ì¸ìŠ¤í„´ìŠ¤ë¡œë¶€í„° ëª¨ë“  ë°ì´í„° ë³µì‚¬"""
        self.db = base.db
        self.device = base.device
        self.metadata_map = base.metadata_map
        self.sbert_movie_ids = base.sbert_movie_ids
        self.sbert_embeddings = base.sbert_embeddings
        self.sbert_movie_to_idx = base.sbert_movie_to_idx
        self.als_movie_to_idx = base.als_movie_to_idx
        self.als_item_factors = base.als_item_factors
        self.common_movie_ids = base.common_movie_ids
        self.movie_id_to_idx = base.movie_id_to_idx
        self.target_sbert_matrix = base.target_sbert_matrix
        self.target_als_matrix = base.target_als_matrix
        self.target_sbert_norm = base.target_sbert_norm
        self.rating_scores = base.rating_scores
        self.movies_by_year = base.movies_by_year
        self.movies_by_genre = base.movies_by_genre
        self.movies_by_ott = base.movies_by_ott
        self.adult_movies = base.adult_movies
        self.non_adult_movies = base.non_adult_movies
        self.movie_ott_map = base.movie_ott_map

    # _get_user_profileì€ ë¶€ëª¨ í´ë˜ìŠ¤ì˜ ê²ƒì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    # í˜„ì¬ ë²„ì „: ê°œë³„ ì„ë² ë”© í–‰ë ¬ ë°˜í™˜ (N, dim) - ìµœëŒ€ ìœ ì‚¬ë„ ë°©ì‹

    def _get_top_movies(
        self,
        user_sbert_profile: np.ndarray,
        user_als_profile: np.ndarray,
        filtered_ids: List[int],
        sbert_weight: float,
        als_weight: float,
        top_k: int = 300,
        exclude_ids: Optional[List[int]] = None,
        preferred_genres: Optional[List[str]] = None
    ) -> List[Dict[str, Any]]:
        """ìƒìœ„ ì˜í™” ì„ ì • - ìµœëŒ€ ìœ ì‚¬ë„ ê³„ì‚°"""
        exclude_ids = exclude_ids or []
        exclude_set = set(exclude_ids)

        # í•„í„°ëœ ì˜í™”ë“¤ì˜ ì¸ë±ìŠ¤
        filtered_indices = []
        for mid in filtered_ids:
            idx = self.movie_id_to_idx.get(mid)
            if idx is not None:
                filtered_indices.append((mid, idx))

        if not filtered_indices:
            return []

        # ìµœëŒ€ ìœ ì‚¬ë„ ê³„ì‚°: (M, dim) @ (dim, N) = (M, N) â†’ max(axis=1)
        indices = [idx for _, idx in filtered_indices]

        # SBERT ìœ ì‚¬ë„: ê° í›„ë³´ ì˜í™”ì™€ ì‚¬ìš©ì ì˜í™”ë“¤ ì¤‘ ìµœëŒ€ ìœ ì‚¬ë„
        sbert_similarities = self.target_sbert_norm[indices] @ user_sbert_profile.T  # (M, N)
        sbert_scores = np.max(sbert_similarities, axis=1)  # (M,) - ìµœëŒ€ê°’ ì‚¬ìš©

        # ALS ìœ ì‚¬ë„: ê° í›„ë³´ ì˜í™”ì™€ ì‚¬ìš©ì ì˜í™”ë“¤ ì¤‘ ìµœëŒ€ ìœ ì‚¬ë„
        als_similarities = self.target_als_matrix[indices] @ user_als_profile.T  # (M, N)
        als_scores = np.max(als_similarities, axis=1)  # (M,) - ìµœëŒ€ê°’ ì‚¬ìš©

        # ë‚˜ë¨¸ì§€ëŠ” ë¶€ëª¨ í´ë˜ìŠ¤ì™€ ë™ì¼ (MinMax ì •ê·œí™”, ìµœì¢… ì ìˆ˜ ê³„ì‚°)
        scaler = MinMaxScaler()

        # í‰ì  ì ìˆ˜ ì¡°íšŒ
        filtered_rating = np.array([self.rating_scores.get(mid, 0.0) for mid, _ in filtered_indices])

        if len(sbert_scores) > 1:
            norm_sbert = scaler.fit_transform(sbert_scores.reshape(-1, 1)).squeeze()
            norm_als = scaler.fit_transform(als_scores.reshape(-1, 1)).squeeze()
            norm_rating = scaler.fit_transform(filtered_rating.reshape(-1, 1)).squeeze()
        else:
            norm_sbert = sbert_scores
            norm_als = als_scores
            norm_rating = filtered_rating

        # ALS ìˆëŠ” ì˜í™” ID ì§‘í•©
        als_ids = set(self.als_movie_to_idx.keys())

        # ìµœì¢… ì ìˆ˜ ê³„ì‚°
        movie_scores = []
        for i, (mid, _) in enumerate(filtered_indices):
            if mid in exclude_set:
                continue

            # ê°€ì¤‘ì¹˜ ì¬ì¡°ì •
            if mid in als_ids:
                model_score = sbert_weight * norm_sbert[i] + als_weight * norm_als[i]
                rec_type = "hybrid"
            else:
                model_score = norm_sbert[i]
                rec_type = "sbert_only"

            rating_score = norm_rating[i] if isinstance(norm_rating, np.ndarray) else norm_rating
            final_score = model_score * 0.7 + rating_score * 0.3

            # ì¥ë¥´ ë¶€ìŠ¤íŠ¸ (Track A only)
            meta = self.metadata_map.get(mid, {})
            if preferred_genres and len(preferred_genres) > 1:
                movie_genres = set(meta.get('genres', []))
                overlap = len(movie_genres & set(preferred_genres))
                if overlap > 0:
                    genre_weight = overlap / len(preferred_genres)
                    genre_boost = genre_weight * 0.15  # ìµœëŒ€ 15% ë¶€ìŠ¤íŠ¸
                    final_score = final_score * (1 + genre_boost)

            movie_scores.append({
                'movie_id': mid,
                'tmdb_id': meta.get('tmdb_id'),
                'title': meta.get('title', 'Unknown'),
                'runtime': meta.get('runtime', 0),
                'genres': meta.get('genres', []),
                'vote_average': meta.get('vote_average', 0),
                'vote_count': meta.get('vote_count', 0),
                'overview': meta.get('overview', ''),
                'release_date': meta.get('release_date', ''),
                'poster_path': meta.get('poster_path', ''),
                'score': final_score,
                'recommendation_type': rec_type
            })

        # ì ìˆ˜ìˆœ ì •ë ¬ í›„ ìƒìœ„ top_k
        movie_scores.sort(key=lambda x: x['score'], reverse=True)
        return movie_scores[:top_k]



class AveragedRecommender(HybridRecommender):
    """í‰ê·  ì„ë² ë”© ë°©ì‹ ì¶”ì²œ ì‹œìŠ¤í…œ (í˜„ì¬ ë²„ì „ ìƒì†)"""

    def __init__(self, db_config: dict = None, als_model_path: str = None,
                 als_data_path: str = None, device: str = None, base_recommender: HybridRecommender = None):
        """
        í˜„ì¬ ë²„ì „ê³¼ ë™ì¼í•˜ì§€ë§Œ í‰ê·  ì„ë² ë”© ë°©ì‹ ì‚¬ìš©

        Args:
            base_recommender: ê¸°ì¡´ HybridRecommender ì¸ìŠ¤í„´ìŠ¤ (ë°ì´í„° ì¬ì‚¬ìš©)
            db_config, als_model_path, als_data_path: base_recommender ì—†ì„ ë•Œ ì‚¬ìš©
        """
        if base_recommender is not None:
            # ê¸°ì¡´ ì¸ìŠ¤í„´ìŠ¤ì˜ ë°ì´í„° ì¬ì‚¬ìš© (ì´ˆê¸°í™” ìŠ¤í‚µ)
            print("  â†’ Reusing data from existing HybridRecommender instance")
            self._copy_from_base(base_recommender)
        else:
            # ìƒˆë¡œ ì´ˆê¸°í™”
            super().__init__(db_config, als_model_path, als_data_path, device)

        print("  â†’ Using AVERAGED EMBEDDING method")

    def _copy_from_base(self, base: HybridRecommender):
        """ê¸°ì¡´ ì¸ìŠ¤í„´ìŠ¤ë¡œë¶€í„° ëª¨ë“  ë°ì´í„° ë³µì‚¬"""
        # DB ì—°ê²°
        self.db = base.db
        self.device = base.device

        # ë©”íƒ€ë°ì´í„°
        self.metadata_map = base.metadata_map

        # SBERT ê´€ë ¨
        self.sbert_movie_ids = base.sbert_movie_ids
        self.sbert_embeddings = base.sbert_embeddings
        self.sbert_movie_to_idx = base.sbert_movie_to_idx

        # ALS ê´€ë ¨
        self.als_movie_to_idx = base.als_movie_to_idx
        self.als_item_factors = base.als_item_factors

        # ê³µí†µ ì˜í™” ë° ì¸ë±ìŠ¤ ë§¤í•‘ (pre-aligned)
        self.common_movie_ids = base.common_movie_ids
        self.movie_id_to_idx = base.movie_id_to_idx
        self.target_sbert_matrix = base.target_sbert_matrix
        self.target_als_matrix = base.target_als_matrix
        self.target_sbert_norm = base.target_sbert_norm

        # í‰ì  ë°ì´í„°
        self.rating_scores = base.rating_scores

        # í•„í„°ë§ìš© ë°ì´í„°
        self.movies_by_year = base.movies_by_year
        self.movies_by_genre = base.movies_by_genre
        self.movies_by_ott = base.movies_by_ott
        self.adult_movies = base.adult_movies
        self.non_adult_movies = base.non_adult_movies

        # OTT ë§¤í•‘
        self.movie_ott_map = base.movie_ott_map

    def _get_user_profile(self, user_movie_ids: List[int]):
        """ì‚¬ìš©ì í”„ë¡œí•„ ë²¡í„° ìƒì„± - í‰ê·  ì„ë² ë”© ë°©ì‹"""
        # SBERT í”„ë¡œí•„
        user_sbert_vecs = []
        for mid in user_movie_ids:
            if mid in self.sbert_movie_to_idx:
                user_sbert_vecs.append(self.sbert_embeddings[self.sbert_movie_to_idx[mid]])

        if not user_sbert_vecs:
            random_ids = list(self.sbert_movie_to_idx.keys())[:5]
            for mid in random_ids:
                user_sbert_vecs.append(self.sbert_embeddings[self.sbert_movie_to_idx[mid]])

        # í‰ê·  ë²¡í„° ê³„ì‚° ë° ì •ê·œí™”
        user_sbert_profile = np.mean(user_sbert_vecs, axis=0)
        user_sbert_profile = user_sbert_profile / (np.linalg.norm(user_sbert_profile) + 1e-10)

        # ALS í”„ë¡œí•„
        user_als_vecs = []
        for mid in user_movie_ids:
            if mid in self.als_movie_to_idx:
                user_als_vecs.append(self.als_item_factors[self.als_movie_to_idx[mid]])

        if not user_als_vecs:
            random_ids = list(self.als_movie_to_idx.keys())[:5]
            for mid in random_ids:
                user_als_vecs.append(self.als_item_factors[self.als_movie_to_idx[mid]])

        # í‰ê·  ë²¡í„° ê³„ì‚° ë° ì •ê·œí™” (ğŸ”§ ìˆ˜ì •: ì •ê·œí™” ì¶”ê°€)
        user_als_profile = np.mean(user_als_vecs, axis=0)
        user_als_profile = user_als_profile / (np.linalg.norm(user_als_profile) + 1e-10)

        return user_sbert_profile, user_als_profile

    def _get_top_movies(
        self,
        user_sbert_profile,
        user_als_profile,
        filtered_ids: List[int],
        sbert_weight: float,
        als_weight: float,
        top_k: int = 300,
        exclude_ids: Optional[List[int]] = None,
        preferred_genres: Optional[List[str]] = None
    ):
        """ìƒìœ„ ì˜í™” ì„ ì • - í‰ê·  ì„ë² ë”© ìœ ì‚¬ë„ ê³„ì‚°"""
        exclude_ids = exclude_ids or []
        exclude_set = set(exclude_ids)

        # í•„í„°ëœ ì˜í™”ë“¤ì˜ ì¸ë±ìŠ¤
        filtered_indices = []
        for mid in filtered_ids:
            idx = self.movie_id_to_idx.get(mid)
            if idx is not None:
                filtered_indices.append((mid, idx))

        if not filtered_indices:
            return []

        # í‰ê·  ë²¡í„° ìœ ì‚¬ë„ ê³„ì‚°: (M, dim) @ (dim,) = (M,)
        indices = [idx for _, idx in filtered_indices]

        sbert_scores = self.target_sbert_norm[indices] @ user_sbert_profile
        als_scores = self.target_als_matrix[indices] @ user_als_profile

        # MinMax ì •ê·œí™”
        scaler = MinMaxScaler()

        # í‰ì  ì ìˆ˜ ì¡°íšŒ
        filtered_rating = np.array([self.rating_scores.get(mid, 0.0) for mid, _ in filtered_indices])

        if len(sbert_scores) > 1:
            norm_sbert = scaler.fit_transform(sbert_scores.reshape(-1, 1)).squeeze()
            norm_als = scaler.fit_transform(als_scores.reshape(-1, 1)).squeeze()
            norm_rating = scaler.fit_transform(filtered_rating.reshape(-1, 1)).squeeze()
        else:
            norm_sbert = sbert_scores
            norm_als = als_scores
            norm_rating = filtered_rating

        # ALS ìˆëŠ” ì˜í™” ID ì§‘í•©
        als_ids = set(self.als_movie_to_idx.keys())

        # ìµœì¢… ì ìˆ˜ ê³„ì‚°
        movie_scores = []
        for i, (mid, _) in enumerate(filtered_indices):
            if mid in exclude_set:
                continue

            # ê°€ì¤‘ì¹˜ ì¬ì¡°ì •
            if mid in als_ids:
                model_score = sbert_weight * norm_sbert[i] + als_weight * norm_als[i]
                rec_type = "hybrid"
            else:
                model_score = norm_sbert[i]
                rec_type = "sbert_only"

            rating_score = norm_rating[i] if isinstance(norm_rating, np.ndarray) else norm_rating
            final_score = model_score * 0.7 + rating_score * 0.3

            meta = self.metadata_map.get(mid, {})
            movie_scores.append({
                'movie_id': mid,
                'tmdb_id': meta.get('tmdb_id'),
                'title': meta.get('title', 'Unknown'),
                'runtime': meta.get('runtime', 0),
                'genres': meta.get('genres', []),
                'vote_average': meta.get('vote_average', 0),
                'vote_count': meta.get('vote_count', 0),
                'overview': meta.get('overview', ''),
                'release_date': meta.get('release_date', ''),
                'poster_path': meta.get('poster_path', ''),
                'score': final_score,
                'recommendation_type': rec_type
            })

        # ì ìˆ˜ìˆœ ì •ë ¬ í›„ ìƒìœ„ top_k
        movie_scores.sort(key=lambda x: x['score'], reverse=True)
        return movie_scores[:top_k]


class MeanSimilarityRecommender(HybridRecommender):
    """í‰ê·  ìœ ì‚¬ë„ ë°©ì‹ ì¶”ì²œ ì‹œìŠ¤í…œ (í˜„ì¬ ë²„ì „ ìƒì†) = í˜„ì¬ í”„ë¡œë•ì…˜ ë²„ì „!

    ê°œë³„ ì˜í™” ì„ë² ë”© ìœ ì§€ â†’ ê° í›„ë³´ ì˜í™”ì™€ ê°œë³„ ìœ ì‚¬ë„ ê³„ì‚° â†’ í‰ê· 
    """

    def __init__(self, db_config: dict = None, als_model_path: str = None,
                 als_data_path: str = None, device: str = None, base_recommender: HybridRecommender = None):
        """
        í˜„ì¬ ë²„ì „ê³¼ ë™ì¼ (í‰ê·  ìœ ì‚¬ë„ ë°©ì‹)

        Args:
            base_recommender: ê¸°ì¡´ HybridRecommender ì¸ìŠ¤í„´ìŠ¤ (ë°ì´í„° ì¬ì‚¬ìš©)
            db_config, als_model_path, als_data_path: base_recommender ì—†ì„ ë•Œ ì‚¬ìš©
        """
        if base_recommender is not None:
            # ê¸°ì¡´ ì¸ìŠ¤í„´ìŠ¤ì˜ ë°ì´í„° ì¬ì‚¬ìš©
            print("  â†’ Reusing data from existing HybridRecommender instance")
            self._copy_from_base(base_recommender)
        else:
            # ìƒˆë¡œ ì´ˆê¸°í™”
            super().__init__(db_config, als_model_path, als_data_path, device)

        print("  â†’ Using MEAN SIMILARITY method (í˜„ì¬ í”„ë¡œë•ì…˜ ë²„ì „)")

    def _copy_from_base(self, base: HybridRecommender):
        """ê¸°ì¡´ ì¸ìŠ¤í„´ìŠ¤ë¡œë¶€í„° ëª¨ë“  ë°ì´í„° ë³µì‚¬"""
        # AveragedRecommenderì™€ ë™ì¼
        self.db = base.db
        self.device = base.device
        self.metadata_map = base.metadata_map
        self.sbert_movie_ids = base.sbert_movie_ids
        self.sbert_embeddings = base.sbert_embeddings
        self.sbert_movie_to_idx = base.sbert_movie_to_idx
        self.als_movie_to_idx = base.als_movie_to_idx
        self.als_item_factors = base.als_item_factors
        self.common_movie_ids = base.common_movie_ids
        self.movie_id_to_idx = base.movie_id_to_idx
        self.target_sbert_matrix = base.target_sbert_matrix
        self.target_als_matrix = base.target_als_matrix
        self.target_sbert_norm = base.target_sbert_norm
        self.rating_scores = base.rating_scores
        self.movies_by_year = base.movies_by_year
        self.movies_by_genre = base.movies_by_genre
        self.movies_by_ott = base.movies_by_ott
        self.adult_movies = base.adult_movies
        self.non_adult_movies = base.non_adult_movies
        self.movie_ott_map = base.movie_ott_map

    # _get_user_profileê³¼ _get_top_moviesëŠ” ë¶€ëª¨ í´ë˜ìŠ¤ì˜ ê²ƒì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    # í˜„ì¬ ë²„ì „: ê°œë³„ ì„ë² ë”© í–‰ë ¬ ë°˜í™˜ (N, dim) + í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°

    def _get_user_profile(self, user_movie_ids: List[int]):
        """ì‚¬ìš©ì í”„ë¡œí•„ ë²¡í„° ìƒì„± - ê°œë³„ ì„ë² ë”© í–‰ë ¬ ë°˜í™˜ (í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°ìš©)"""
        # SBERT í”„ë¡œí•„ (ê°œë³„ ì„ë² ë”© ìœ ì§€)
        user_sbert_vecs = []
        for mid in user_movie_ids:
            if mid in self.sbert_movie_to_idx:
                user_sbert_vecs.append(self.sbert_embeddings[self.sbert_movie_to_idx[mid]])

        if not user_sbert_vecs:
            random_ids = list(self.sbert_movie_to_idx.keys())[:5]
            for mid in random_ids:
                user_sbert_vecs.append(self.sbert_embeddings[self.sbert_movie_to_idx[mid]])

        # í–‰ë ¬ë¡œ ë³€í™˜ ë° ì •ê·œí™” (N, SBERT_dim)
        user_sbert_matrix = np.array(user_sbert_vecs)
        user_sbert_matrix = user_sbert_matrix / (
            np.linalg.norm(user_sbert_matrix, axis=1, keepdims=True) + 1e-10
        )

        # ALS í”„ë¡œí•„ (ê°œë³„ ì„ë² ë”© ìœ ì§€)
        user_als_vecs = []
        for mid in user_movie_ids:
            if mid in self.als_movie_to_idx:
                user_als_vecs.append(self.als_item_factors[self.als_movie_to_idx[mid]])

        if not user_als_vecs:
            random_ids = list(self.als_movie_to_idx.keys())[:5]
            for mid in random_ids:
                user_als_vecs.append(self.als_item_factors[self.als_movie_to_idx[mid]])

        # í–‰ë ¬ë¡œ ë³€í™˜ ë° ì •ê·œí™” (N, ALS_dim)
        user_als_matrix = np.array(user_als_vecs)
        user_als_matrix = user_als_matrix / (
            np.linalg.norm(user_als_matrix, axis=1, keepdims=True) + 1e-10
        )

        return user_sbert_matrix, user_als_matrix

    def _get_top_movies(
        self,
        user_sbert_profile,
        user_als_profile,
        filtered_ids: List[int],
        sbert_weight: float,
        als_weight: float,
        top_k: int = 300,
        exclude_ids: Optional[List[int]] = None,
        preferred_genres: Optional[List[str]] = None
    ):
        """ìƒìœ„ ì˜í™” ì„ ì • - í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°"""
        exclude_ids = exclude_ids or []
        exclude_set = set(exclude_ids)

        # í•„í„°ëœ ì˜í™”ë“¤ì˜ ì¸ë±ìŠ¤
        filtered_indices = []
        for mid in filtered_ids:
            idx = self.movie_id_to_idx.get(mid)
            if idx is not None:
                filtered_indices.append((mid, idx))

        if not filtered_indices:
            return []

        # í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°: (M, dim) @ (dim, N) = (M, N) â†’ mean(axis=1)
        indices = [idx for _, idx in filtered_indices]

        # SBERT ìœ ì‚¬ë„: ê° í›„ë³´ ì˜í™”ì™€ ì‚¬ìš©ì ì˜í™”ë“¤ì˜ ìœ ì‚¬ë„ í‰ê· 
        sbert_similarities = self.target_sbert_norm[indices] @ user_sbert_profile.T  # (M, N)
        sbert_scores = np.mean(sbert_similarities, axis=1)  # (M,)

        # ALS ìœ ì‚¬ë„: ê° í›„ë³´ ì˜í™”ì™€ ì‚¬ìš©ì ì˜í™”ë“¤ì˜ ìœ ì‚¬ë„ í‰ê· 
        als_similarities = self.target_als_matrix[indices] @ user_als_profile.T  # (M, N)
        als_scores = np.mean(als_similarities, axis=1)  # (M,)

        # ë‚˜ë¨¸ì§€ëŠ” í˜„ì¬ ë²„ì „ê³¼ ë™ì¼ (MinMax ì •ê·œí™”, ìµœì¢… ì ìˆ˜ ê³„ì‚°)
        scaler = MinMaxScaler()

        # í‰ì  ì ìˆ˜ ì¡°íšŒ
        filtered_rating = np.array([self.rating_scores.get(mid, 0.0) for mid, _ in filtered_indices])

        if len(sbert_scores) > 1:
            norm_sbert = scaler.fit_transform(sbert_scores.reshape(-1, 1)).squeeze()
            norm_als = scaler.fit_transform(als_scores.reshape(-1, 1)).squeeze()
            norm_rating = scaler.fit_transform(filtered_rating.reshape(-1, 1)).squeeze()
        else:
            norm_sbert = sbert_scores
            norm_als = als_scores
            norm_rating = filtered_rating

        # ALS ìˆëŠ” ì˜í™” ID ì§‘í•©
        als_ids = set(self.als_movie_to_idx.keys())

        # ìµœì¢… ì ìˆ˜ ê³„ì‚°
        movie_scores = []
        for i, (mid, _) in enumerate(filtered_indices):
            if mid in exclude_set:
                continue

            # ê°€ì¤‘ì¹˜ ì¬ì¡°ì •
            if mid in als_ids:
                model_score = sbert_weight * norm_sbert[i] + als_weight * norm_als[i]
                rec_type = "hybrid"
            else:
                model_score = norm_sbert[i]
                rec_type = "sbert_only"

            rating_score = norm_rating[i] if isinstance(norm_rating, np.ndarray) else norm_rating
            final_score = model_score * 0.7 + rating_score * 0.3

            meta = self.metadata_map.get(mid, {})
            movie_scores.append({
                'movie_id': mid,
                'tmdb_id': meta.get('tmdb_id'),
                'title': meta.get('title', 'Unknown'),
                'runtime': meta.get('runtime', 0),
                'genres': meta.get('genres', []),
                'vote_average': meta.get('vote_average', 0),
                'vote_count': meta.get('vote_count', 0),
                'overview': meta.get('overview', ''),
                'release_date': meta.get('release_date', ''),
                'poster_path': meta.get('poster_path', ''),
                'score': final_score,
                'recommendation_type': rec_type
            })

        # ì ìˆ˜ìˆœ ì •ë ¬ í›„ ìƒìœ„ top_k
        movie_scores.sort(key=lambda x: x['score'], reverse=True)
        return movie_scores[:top_k]



class RecommenderEvaluator:
    """ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ í‰ê°€ í´ë˜ìŠ¤"""

    def __init__(self, db_config: Dict):
        self.db_config = db_config
        # í˜„ì¬ ë²„ì „ì—ì„œëŠ” DB ì—°ê²°ì„ HybridRecommenderê°€ ì§ì ‘ ê´€ë¦¬

    def get_test_users_from_ratings_csv(
        self,
        ratings_csv_path: str,
        recommender: HybridRecommender,
        min_ratings: int = 20,
        num_users: int = 100
    ) -> List[Tuple[int, List[int], List[int]]]:
        """
        ratings.csv íŒŒì¼ì—ì„œ í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì ë°ì´í„° ì¶”ì¶œ

        Args:
            ratings_csv_path: ratings.csv íŒŒì¼ ê²½ë¡œ
            recommender: ì¶”ì²œ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ (DB í™•ì¸ìš©)
            min_ratings: ìµœì†Œ í‰ì  ê°œìˆ˜ (DB ì¡´ì¬ ì˜í™” ê¸°ì¤€)
            num_users: ìµœëŒ€ ì‚¬ìš©ì ìˆ˜

        Returns:
            [(user_id, train_movies, test_movies), ...]
        """
        import pandas as pd

        print(f"\nğŸ“Š ratings.csvì—ì„œ í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì ì¶”ì¶œ ì¤‘...")
        print(f"  íŒŒì¼: {ratings_csv_path}")
        print(f"  ì¡°ê±´: ìµœì†Œ {min_ratings}ê°œ í‰ê°€ (DB ì¡´ì¬ ì˜í™” ê¸°ì¤€), ìµœëŒ€ {num_users}ëª…")

        try:
            # ratings.csv ë¡œë“œ
            df = pd.read_csv(ratings_csv_path)
            print(f"  ì „ì²´ í‰ì  ë°ì´í„°: {len(df):,}ê°œ")
            print(f"  ì „ì²´ ì‚¬ìš©ì: {df['userId'].nunique():,}ëª…")
            print(f"  ì „ì²´ ì˜í™”: {df['movieId'].nunique():,}ê°œ")

            # ğŸ”§ ê°œì„  1: DBì— ì¡´ì¬í•˜ëŠ” ì˜í™”ë§Œ í•„í„°ë§
            valid_movie_ids = set(recommender.metadata_map.keys())
            df_valid = df[df['movieId'].isin(valid_movie_ids)].copy()

            print(f"  DBì— ì¡´ì¬í•˜ëŠ” í‰ì : {len(df_valid):,}ê°œ ({len(df_valid)/len(df)*100:.1f}%)")
            print(f"  DBì— ì¡´ì¬í•˜ëŠ” ì˜í™”: {df_valid['movieId'].nunique():,}ê°œ")

            # ğŸ”§ ê°œì„  2: 2000ë…„ ì´ìƒ, ë¹„ì„±ì¸ë¬¼ë§Œ í•„í„°ë§
            valid_filtered_ids = set()
            for mid in df_valid['movieId'].unique():
                meta = recommender.metadata_map.get(mid, {})

                # 2000ë…„ ì´ìƒ ì²´í¬
                release_date = meta.get('release_date', '')
                if release_date:
                    try:
                        year = int(release_date[:4])
                        if year < 2000:
                            continue
                    except:
                        continue

                # ë¹„ì„±ì¸ë¬¼ ì²´í¬
                if meta.get('adult', False):
                    continue

                valid_filtered_ids.add(mid)

            df_filtered = df_valid[df_valid['movieId'].isin(valid_filtered_ids)].copy()
            print(f"  í•„í„°ë§ í›„ í‰ì : {len(df_filtered):,}ê°œ ({len(df_filtered)/len(df)*100:.1f}%)")
            print(f"  í•„í„°ë§ í›„ ì˜í™”: {df_filtered['movieId'].nunique():,}ê°œ")

            # ğŸ”§ ê°œì„  3: í•„í„°ë§ëœ ë°ì´í„°ë¡œ ì‚¬ìš©ì ê°œìˆ˜ ê³„ì‚°
            user_counts = df_filtered.groupby('userId').size()
            valid_users = user_counts[user_counts >= min_ratings].index.tolist()
            print(f"  {min_ratings}ê°œ ì´ìƒ í‰ê°€í•œ ì‚¬ìš©ì (í•„í„°ë§ í›„): {len(valid_users):,}ëª…")

            # ìƒìœ„ num_users ëª… ì„ íƒ (í•„í„°ë§ëœ í‰ì  ë§ì€ ìˆœ)
            top_users = user_counts.nlargest(num_users).index.tolist()

            test_users = []
            skipped_stats = {
                'insufficient_train': 0,
                'insufficient_test': 0
            }

            for user_id in top_users:
                # ì‚¬ìš©ìì˜ í•„í„°ë§ëœ í‰ì  ê°€ì ¸ì˜¤ê¸° (ì‹œê°„ìˆœ ì •ë ¬)
                user_ratings = df_filtered[df_filtered['userId'] == user_id].sort_values('timestamp')

                # ì˜í™” ID ë¦¬ìŠ¤íŠ¸ (ì´ë¯¸ í•„í„°ë§ë¨)
                movies = user_ratings['movieId'].tolist()

                # 80% train, 20% test ë¶„í• 
                split_idx = int(len(movies) * 0.8)
                train = movies[:split_idx]
                test = movies[split_idx:]

                # ìµœì†Œ ì¡°ê±´: train >= 15ê°œ, test >= 5ê°œ (ì—¬ìœ  ìˆê²Œ)
                if len(train) < 15:
                    skipped_stats['insufficient_train'] += 1
                    continue

                if len(test) < 5:
                    skipped_stats['insufficient_test'] += 1
                    continue

                test_users.append((user_id, train, test))

            print(f"âœ… {len(test_users)}ëª…ì˜ í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì ì¶”ì¶œ ì™„ë£Œ")

            if skipped_stats['insufficient_train'] > 0 or skipped_stats['insufficient_test'] > 0:
                print(f"  ìŠ¤í‚µëœ ì‚¬ìš©ì:")
                print(f"    - Train ë¶€ì¡±: {skipped_stats['insufficient_train']}ëª…")
                print(f"    - Test ë¶€ì¡±: {skipped_stats['insufficient_test']}ëª…")

            if len(test_users) > 0:
                avg_train = sum(len(t) for _, t, _ in test_users) / len(test_users)
                avg_test = sum(len(t) for _, _, t in test_users) / len(test_users)
                print(f"  í‰ê·  train ì˜í™” ìˆ˜: {avg_train:.1f}ê°œ")
                print(f"  í‰ê·  test ì˜í™” ìˆ˜: {avg_test:.1f}ê°œ")

            return test_users

        except Exception as e:
            print(f"âŒ ratings.csv ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return []

    def filter_valid_movies(
        self,
        movie_ids: List[int],
        recommender: HybridRecommender
    ) -> Tuple[List[int], Dict[str, int]]:
        """
        DBì— ì¡´ì¬í•˜ê³  ì¶”ì²œ ê°€ëŠ¥í•œ ì˜í™”ë§Œ í•„í„°ë§

        Args:
            movie_ids: ì›ë³¸ ì˜í™” ID ë¦¬ìŠ¤íŠ¸
            recommender: ì¶”ì²œ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤

        Returns:
            (valid_movies, stats)
            - valid_movies: ìœ íš¨í•œ ì˜í™” ID ë¦¬ìŠ¤íŠ¸
            - stats: {'total': ì›ë³¸ ê°œìˆ˜, 'not_in_db': DBì— ì—†ëŠ” ê°œìˆ˜, 'filtered': í•„í„°ë§ëœ ê°œìˆ˜, 'valid': ìœ íš¨í•œ ê°œìˆ˜}
        """
        stats = {
            'total': len(movie_ids),
            'not_in_db': 0,
            'filtered': 0,
            'valid': 0
        }

        valid_movies = []

        for mid in movie_ids:
            # DBì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if mid not in recommender.metadata_map:
                stats['not_in_db'] += 1
                continue

            meta = recommender.metadata_map[mid]

            # ì¶”ì²œ ê°€ëŠ¥ ì¡°ê±´ ì²´í¬ (2000ë…„ ì´ìƒ, ë¹„ì„±ì¸ë¬¼)
            release_date = meta.get('release_date', '')
            if release_date:
                try:
                    year = int(release_date[:4])
                    if year < 2000:
                        stats['filtered'] += 1
                        continue
                except:
                    stats['filtered'] += 1
                    continue

            if meta.get('adult', False):
                stats['filtered'] += 1
                continue

            valid_movies.append(mid)
            stats['valid'] += 1

        return valid_movies, stats

    def calculate_precision_at_k(
        self,
        recommendations: List[int],
        ground_truth: List[int],
        k: int = 10
    ) -> float:
        """Precision@K ê³„ì‚°"""
        top_k = recommendations[:k]
        relevant = set(ground_truth)
        hits = len(set(top_k) & relevant)
        return hits / k if k > 0 else 0.0

    def calculate_ndcg_at_k(
        self,
        recommendations: List[int],
        ground_truth: List[int],
        k: int = 10
    ) -> float:
        """NDCG@K ê³„ì‚°
        
        NDCG = DCG / IDCG
        - DCG: ì¶”ì²œ ìˆœì„œëŒ€ë¡œì˜ ëˆ„ì  ì´ë“
        - IDCG: ì´ìƒì ì¸ ìˆœì„œ(ì •ë ¬)ì˜ ëˆ„ì  ì´ë“
        """
        top_k = recommendations[:k]
        relevant = set(ground_truth)

        # ì¶”ì²œ ìˆœì„œëŒ€ë¡œì˜ ê´€ë ¨ë„ (0 or 1)
        relevance = np.array([1.0 if movie_id in relevant else 0.0 for movie_id in top_k])
        
        # ê´€ë ¨ ì˜í™”ê°€ ì—†ìœ¼ë©´ 0 ë°˜í™˜
        if relevance.sum() == 0:
            return 0.0

        # DCG ê³„ì‚° (Discounted Cumulative Gain)
        # DCG = sum(rel_i / log2(i + 1)) for i in 1..k
        dcg = 0.0
        for i, rel in enumerate(relevance):
            dcg += rel / np.log2(i + 2)  # i+2 because i starts at 0
        
        # IDCG ê³„ì‚° (Ideal DCG - ê´€ë ¨ë„ ìˆœìœ¼ë¡œ ì •ë ¬)
        ideal_relevance = np.sort(relevance)[::-1]  # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        idcg = 0.0
        for i, rel in enumerate(ideal_relevance):
            idcg += rel / np.log2(i + 2)
        
        # NDCG = DCG / IDCG
        return dcg / idcg if idcg > 0 else 0.0

    def calculate_recall_at_k(
        self,
        recommendations: List[int],
        ground_truth: List[int],
        k: int = 10
    ) -> float:
        """Recall@K ê³„ì‚°

        ì¶”ì²œí•œ Kê°œ ì¤‘ ì‹¤ì œë¡œ ì‚¬ìš©ìê°€ ë³¸ ì˜í™”ì˜ ë¹„ìœ¨ (ì „ì²´ í…ŒìŠ¤íŠ¸ ì˜í™” ëŒ€ë¹„)
        """
        top_k = recommendations[:k]
        relevant = set(ground_truth)
        hits = len(set(top_k) & relevant)
        return hits / len(relevant) if len(relevant) > 0 else 0.0

    def calculate_diversity(
        self,
        recommendations: List[Dict[str, Any]]
    ) -> float:
        """ì¥ë¥´ ë‹¤ì–‘ì„± ê³„ì‚°"""
        all_genres = []
        for movie in recommendations:
            genres = movie.get('genres', [])
            all_genres.extend(genres)

        if len(all_genres) == 0:
            return 0.0

        unique_genres = len(set(all_genres))
        total_genres = len(all_genres)

        return unique_genres / total_genres

    def evaluate_recommender(
        self,
        recommender: HybridRecommender,
        test_users: List[Tuple[str, List[int], List[int]]],
        method_name: str,
        k: int = 10
    ) -> Dict[str, Any]:
        """
        ì¶”ì²œ ì‹œìŠ¤í…œ í‰ê°€ (Top-K ì§ì ‘ í‰ê°€ ë°©ì‹)

        Args:
            recommender: ì¶”ì²œ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
            test_users: í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì ë¦¬ìŠ¤íŠ¸
            method_name: ë°©ë²• ì´ë¦„
            k: í‰ê°€í•  ìƒìœ„ Kê°œ

        Returns:
            í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        precision_scores = []
        recall_scores = []
        ndcg_scores = []
        diversity_scores = []
        elapsed_times = []

        # í†µê³„ ì¶”ì 
        total_stats = {
            'users_evaluated': 0,
            'users_skipped': 0,
            'train_not_in_db': 0,
            'train_filtered': 0,
            'test_not_in_db': 0,
            'test_filtered': 0
        }

        print(f"\n{'='*60}")
        print(f"í‰ê°€ ì‹œì‘: {method_name}")
        print(f"{'='*60}")

        for idx, (user_id, train_movies, test_movies) in enumerate(test_users):
            # ì¶”ì²œ ì‹œê°„ ì¸¡ì •
            start_time = time.time()

            try:
                # 1. ìœ ì € í”„ë¡œí•„ ìƒì„±
                user_sbert_profile, user_als_profile = recommender._get_user_profile(train_movies)

                # 2. ëª¨ë“  ì˜í™” ID ê°€ì ¸ì˜¤ê¸° (í•„í„°ë§ ì—†ìŒ)
                all_movie_ids = list(recommender.metadata_map.keys())
                
                # 3. Train ì˜í™”ë§Œ ì œì™¸ (ì´ë¯¸ ë³¸ ì˜í™”)
                candidate_ids = [mid for mid in all_movie_ids if mid not in train_movies]
                
                # 4. ìˆœìˆ˜ ì„ë² ë”© ìœ ì‚¬ë„ ê³„ì‚°
                top_movies = self._get_top_movies_pure(
                    recommender=recommender,
                    user_sbert_profile=user_sbert_profile,
                    user_als_profile=user_als_profile,
                    candidate_ids=candidate_ids,
                    sbert_weight=0.7,
                    als_weight=0.3,
                    top_k=k
                )

                elapsed = time.time() - start_time
                elapsed_times.append(elapsed)

                # ìƒìœ„ Kê°œ ì˜í™” ID ì¶”ì¶œ
                top_k_ids = [m['movie_id'] for m in top_movies[:k]]

                # Precision@K ê³„ì‚°
                precision = self.calculate_precision_at_k(top_k_ids, test_movies, k)
                precision_scores.append(precision)

                # Recall@K ê³„ì‚°
                recall = self.calculate_recall_at_k(top_k_ids, test_movies, k)
                recall_scores.append(recall)

                # NDCG@K ê³„ì‚°
                ndcg = self.calculate_ndcg_at_k(top_k_ids, test_movies, k)
                ndcg_scores.append(ndcg)

                # Diversity ê³„ì‚°
                diversity = self.calculate_diversity(top_movies[:k])
                diversity_scores.append(diversity)

                total_stats['users_evaluated'] += 1

                # ì§„í–‰ ìƒí™© ì¶œë ¥ (ë§¤ 10ëª…ë§ˆë‹¤)
                if (idx + 1) % 10 == 0:
                    print(f"  ì§„í–‰: {idx + 1}/{len(test_users)} users (í‰ê°€ ì™„ë£Œ: {total_stats['users_evaluated']}ëª…)", flush=True)

            except Exception as e:
                total_stats['users_skipped'] += 1
                print(f"  âš ï¸ User {user_id} í‰ê°€ ì‹¤íŒ¨: {e}", flush=True)
                import traceback
                traceback.print_exc()
                continue

        # ê²°ê³¼ ì§‘ê³„
        results = {
            'precision@10': np.mean(precision_scores) if precision_scores else 0.0,
            'recall@10': np.mean(recall_scores) if recall_scores else 0.0,
            'ndcg@10': np.mean(ndcg_scores) if ndcg_scores else 0.0,
            'diversity': np.mean(diversity_scores) if diversity_scores else 0.0,
            'avg_time': np.mean(elapsed_times) if elapsed_times else 0.0,
            'std_time': np.std(elapsed_times) if elapsed_times else 0.0,
            'num_users': len(precision_scores),
            # ì›ë³¸ ì ìˆ˜ ì €ì¥ (í†µê³„ ê²€ì¦ìš©)
            'precision_scores': precision_scores,
            'recall_scores': recall_scores,
            'ndcg_scores': ndcg_scores,
            'diversity_scores': diversity_scores,
            'elapsed_times': elapsed_times,
            # í†µê³„ ì •ë³´
            'stats': total_stats
        }

        print(f"\nğŸ“Š í‰ê°€ ê²°ê³¼ ({method_name}):", flush=True)
        print(f"  Precision@{k}: {results['precision@10']:.4f}", flush=True)
        print(f"  Recall@{k}: {results['recall@10']:.4f}", flush=True)
        print(f"  NDCG@{k}: {results['ndcg@10']:.4f}", flush=True)
        print(f"  Diversity: {results['diversity']:.4f}", flush=True)
        print(f"  í‰ê·  ì¶”ì²œ ì‹œê°„: {results['avg_time']:.3f}s (Â±{results['std_time']:.3f}s)", flush=True)
        print(f"  í‰ê°€ ì‚¬ìš©ì ìˆ˜: {results['num_users']}ëª…", flush=True)

        # í‰ê°€ í†µê³„ ì¶œë ¥
        print(f"\nğŸ“ˆ í‰ê°€ í†µê³„:", flush=True)
        print(f"  í‰ê°€ ì™„ë£Œ: {total_stats['users_evaluated']}ëª…", flush=True)
        print(f"  ìŠ¤í‚µ: {total_stats['users_skipped']}ëª…", flush=True)

        return results
    
    def _get_top_movies_pure(
        self,
        recommender: HybridRecommender,
        user_sbert_profile: np.ndarray,
        user_als_profile: np.ndarray,
        candidate_ids: List[int],
        sbert_weight: float,
        als_weight: float,
        top_k: int
    ) -> List[Dict[str, Any]]:
        """ìˆœìˆ˜ ì„ë² ë”© ìœ ì‚¬ë„ë§Œìœ¼ë¡œ ìƒìœ„ ì˜í™” ì„ ì •
        
        - í•„í„°ë§ ì—†ìŒ
        - ì¥ë¥´ ë¶€ìŠ¤íŠ¸ ì—†ìŒ
        - í‰ì  ì ìˆ˜ ì—†ìŒ
        - ì •ê·œí™”ë§Œ ì‚¬ìš© (SBERT + ALS ê³µì • ê²°í•©)
        """
        # í›„ë³´ ì˜í™”ë“¤ì˜ ì¸ë±ìŠ¤
        candidate_indices = []
        for mid in candidate_ids:
            idx = recommender.movie_id_to_idx.get(mid)
            if idx is not None:
                candidate_indices.append((mid, idx))
        
        if not candidate_indices:
            return []
        
        indices = [idx for _, idx in candidate_indices]

        # ALS íƒ€ê²Ÿ í–‰ë ¬ ì •ê·œí™” (SBERTì™€ ë™ì¼í•œ ìŠ¤ì¼€ì¼ë¡œ)
        target_als_norm = recommender.target_als_matrix / (
            np.linalg.norm(recommender.target_als_matrix, axis=1, keepdims=True) + 1e-10
        )

        # ìœ ì‚¬ë„ ê³„ì‚° (í”„ë¡œí•„ ë°©ì‹ì— ë”°ë¼ ë‹¤ë¦„)
        if isinstance(recommender, AveragedRecommender):
            # í‰ê·  ì„ë² ë”© ë°©ì‹: (M, dim) @ (dim,) = (M,)
            sbert_scores = recommender.target_sbert_norm[indices] @ user_sbert_profile
            als_scores = target_als_norm[indices] @ user_als_profile
        else:
            # ìµœëŒ€/í‰ê·  ìœ ì‚¬ë„ ë°©ì‹: (M, dim) @ (dim, N) = (M, N)
            sbert_similarities = recommender.target_sbert_norm[indices] @ user_sbert_profile.T
            als_similarities = target_als_norm[indices] @ user_als_profile.T

            # ìµœëŒ€ ë˜ëŠ” í‰ê·  (ë¨¼ì € ê³„ì‚°)
            if isinstance(recommender, MaxSimilarityRecommender):
                sbert_scores = np.max(sbert_similarities, axis=1)
                als_scores = np.max(als_similarities, axis=1)
            elif isinstance(recommender, MeanSimilarityRecommender):
                sbert_scores = np.mean(sbert_similarities, axis=1)
                als_scores = np.mean(als_similarities, axis=1)
            else:
                raise ValueError(f"Unknown recommender type: {type(recommender)}")

            # í”„ë¡œë•ì…˜ê³¼ ë™ì¼: SBERTì™€ ALS ë”°ë¡œ ì •ê·œí™” í›„ ê²°í•©
            scaler = MinMaxScaler()

            if len(sbert_scores) > 1:
                norm_sbert = scaler.fit_transform(sbert_scores.reshape(-1, 1)).squeeze()
                norm_als = scaler.fit_transform(als_scores.reshape(-1, 1)).squeeze()
            else:
                norm_sbert = sbert_scores
                norm_als = als_scores

            # ALS ìˆëŠ” ì˜í™” ID ì§‘í•©
            als_ids = set(recommender.als_movie_to_idx.keys())

            # ìµœì¢… ì ìˆ˜ ê³„ì‚° (í”„ë¡œë•ì…˜ê³¼ ë™ì¼ ë¡œì§)
            final_scores = np.zeros(len(candidate_indices))
            for i, (mid, _) in enumerate(candidate_indices):
                if mid in als_ids:
                    final_scores[i] = sbert_weight * norm_sbert[i] + als_weight * norm_als[i]
                else:
                    final_scores[i] = norm_sbert[i]

            # í‰ê°€ìš©ì´ë¯€ë¡œ sbert_scoresë¥¼ Noneìœ¼ë¡œ (í‰ê·  ì„ë² ë”© ë¶„ê¸°ì™€ êµ¬ë¶„)
            sbert_scores = None
            als_scores = None
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚°
        movie_scores = []
        
        # í‰ê·  ì„ë² ë”© ë°©ì‹: SBERTì™€ ALS ë”°ë¡œ ì •ê·œí™” í›„ ê²°í•©
        if sbert_scores is not None:
            # MinMax ì •ê·œí™” (SBERT + ALS ê³µì • ê²°í•©)
            scaler = MinMaxScaler()
            
            if len(sbert_scores) > 1:
                norm_sbert = scaler.fit_transform(sbert_scores.reshape(-1, 1)).squeeze()
                norm_als = scaler.fit_transform(als_scores.reshape(-1, 1)).squeeze()
            else:
                norm_sbert = sbert_scores
                norm_als = als_scores
            
            # ALS ìˆëŠ” ì˜í™” ID ì§‘í•©
            als_ids = set(recommender.als_movie_to_idx.keys())
            
            # ìµœì¢… ì ìˆ˜ ê³„ì‚° (ìˆœìˆ˜ ì„ë² ë”©ë§Œ)
            for i, (mid, _) in enumerate(candidate_indices):
                # ALS ìˆìœ¼ë©´ í•˜ì´ë¸Œë¦¬ë“œ, ì—†ìœ¼ë©´ SBERTë§Œ
                if mid in als_ids:
                    final_score = sbert_weight * norm_sbert[i] + als_weight * norm_als[i]
                else:
                    final_score = norm_sbert[i]
                
                meta = recommender.metadata_map.get(mid, {})
                movie_scores.append({
                    'movie_id': mid,
                    'title': meta.get('title', 'Unknown'),
                    'genres': meta.get('genres', []),
                    'score': final_score
                })
        
        # ìµœëŒ€/í‰ê·  ìœ ì‚¬ë„ ë°©ì‹: ì´ë¯¸ final_scores ê³„ì‚°ë¨
        else:
            for i, (mid, _) in enumerate(candidate_indices):
                meta = recommender.metadata_map.get(mid, {})
                movie_scores.append({
                    'movie_id': mid,
                    'title': meta.get('title', 'Unknown'),
                    'genres': meta.get('genres', []),
                    'score': final_scores[i]
                })
        
        # ì ìˆ˜ìˆœ ì •ë ¬ í›„ ìƒìœ„ top_k
        movie_scores.sort(key=lambda x: x['score'], reverse=True)
        return movie_scores[:top_k]


def interpret_effect_size(d):
    """Cohen's d íš¨ê³¼ í¬ê¸° í•´ì„"""
    abs_d = abs(d)
    if abs_d < 0.2:
        return "ë§¤ìš° ì‘ìŒ"
    elif abs_d < 0.5:
        return "ì‘ìŒ"
    elif abs_d < 0.8:
        return "ì¤‘ê°„"
    else:
        return "í¼"


def main():
    """ë©”ì¸ í‰ê°€ í•¨ìˆ˜"""

    # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •
    set_seed(42)

    load_dotenv()

    # DB ì„¤ì •
    DB_CONFIG = {
        'host': os.getenv("DATABASE_HOST", "localhost"),
        'port': int(os.getenv("DATABASE_PORT", 5432)),
        'database': os.getenv("DATABASE_NAME", "moviesir"),
        'user': os.getenv("DATABASE_USER", "movigation"),
        'password': os.getenv("DATABASE_PASSWORD", "moviesir123")
    }

    # ëª¨ë¸ ê²½ë¡œ (ai/ í´ë” ê¸°ì¤€)
    current_dir = Path(__file__).parent.parent.parent  # ai/ í´ë”
    ALS_MODEL_PATH = str(current_dir / "training/als_data")
    ALS_DATA_PATH = str(current_dir / "training/als_data")
    RATINGS_CSV_PATH = str(current_dir / "training/original_data/ratings.csv")

    print("\n" + "="*60)
    print("ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ í‰ê°€ ë° ë¹„êµ")
    print("í‰ê·  ì„ë² ë”© vs ìµœëŒ€ ìœ ì‚¬ë„")
    print("="*60)

    # 1. ê¸°ë³¸ ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ë°ì´í„° ë¡œë”© ë° í•„í„°ë§ìš©)
    print("\n" + "="*60)
    print("ğŸ“¦ ì¶”ì²œ ì‹œìŠ¤í…œ ë°ì´í„° ì´ˆê¸°í™” ì¤‘...")
    print("="*60)

    base_recommender = HybridRecommender(
        db_config=DB_CONFIG,
        als_model_path=ALS_MODEL_PATH,
        als_data_path=ALS_DATA_PATH
    )
    print("âœ… ì´ˆê¸°í™” ì™„ë£Œ (ì´ ë°ì´í„°ë¥¼ 3ê°€ì§€ ë°©ì‹ ëª¨ë‘ì—ì„œ ì¬ì‚¬ìš©)\n")

    # 2. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ (ratings.csv ì‚¬ìš©, í•„í„°ë§ í¬í•¨)
    evaluator = RecommenderEvaluator(DB_CONFIG)
    test_users = evaluator.get_test_users_from_ratings_csv(
        ratings_csv_path=RATINGS_CSV_PATH,
        recommender=base_recommender,  # í•„í„°ë§ì— ì‚¬ìš©
        min_ratings=20,
        num_users=100
    )

    if len(test_users) == 0:
        print("âš ï¸ í…ŒìŠ¤íŠ¸ ì‚¬ìš©ìê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("\nğŸ’¡ í•´ê²° ë°©ë²•:")
        print("   1. ratings.csv íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸")
        print(f"   2. ê²½ë¡œ í™•ì¸: {RATINGS_CSV_PATH}")
        base_recommender.close()
        return

    print(f"\nâœ… ì´ {len(test_users)}ëª…ì˜ í…ŒìŠ¤íŠ¸ ì‚¬ìš©ìë¡œ í‰ê°€ ì§„í–‰")

    # 3. ìµœëŒ€ ìœ ì‚¬ë„ ë°©ì‹ í‰ê°€
    print("\n" + "="*60)
    print("1ï¸âƒ£ ìµœëŒ€ ìœ ì‚¬ë„ ë°©ì‹ í‰ê°€ ì‹œì‘...")
    print("="*60)

    recommender_max = MaxSimilarityRecommender(base_recommender=base_recommender)

    results_max = evaluator.evaluate_recommender(
        recommender=recommender_max,
        test_users=test_users,
        method_name="ìµœëŒ€ ìœ ì‚¬ë„ ë°©ì‹ (Max Similarity)",
        k=10
    )

    # 4. í‰ê·  ì„ë² ë”© ë°©ì‹ í‰ê°€
    print("\n" + "="*60)
    print("2ï¸âƒ£ í‰ê·  ì„ë² ë”© ë°©ì‹ í‰ê°€ ì‹œì‘...")
    print("="*60)

    recommender_avg = AveragedRecommender(base_recommender=base_recommender)

    results_avg = evaluator.evaluate_recommender(
        recommender=recommender_avg,
        test_users=test_users,
        method_name="í‰ê·  ì„ë² ë”© ë°©ì‹ (Averaged Embedding)",
        k=10
    )

    # 5. í‰ê·  ìœ ì‚¬ë„ ë°©ì‹ í‰ê°€ (í˜„ì¬ í”„ë¡œë•ì…˜ ë²„ì „)
    print("\n" + "="*60)
    print("3ï¸âƒ£ í‰ê·  ìœ ì‚¬ë„ ë°©ì‹ í‰ê°€ ì‹œì‘ (í˜„ì¬ í”„ë¡œë•ì…˜ ë²„ì „)...")
    print("="*60)

    recommender_mean = MeanSimilarityRecommender(base_recommender=base_recommender)

    results_mean = evaluator.evaluate_recommender(
        recommender=recommender_mean,
        test_users=test_users,
        method_name="í‰ê·  ìœ ì‚¬ë„ ë°©ì‹ (Mean Similarity - í˜„ì¬)",
        k=10
    )

    # 6. ì •ë¦¬ (DB ì—°ê²°ì€ base_recommenderê°€ ê´€ë¦¬í•˜ë¯€ë¡œ í•œ ë²ˆë§Œ close)
    base_recommender.close()

    # 7. í†µê³„ì  ìœ ì˜ì„± ê²€ì¦
    print("\n" + "="*60)
    print("ğŸ“ˆ í†µê³„ì  ìœ ì˜ì„± ê²€ì¦")
    print("="*60)

    # í‰ê°€ëœ ì‚¬ìš©ì ìˆ˜ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìµœì†Œ ê¸¸ì´ë¡œ ë§ì¶¤
    min_len = min(len(results_max['precision_scores']), len(results_avg['precision_scores']))

    if min_len < 2:
        print("âš ï¸ í†µê³„ ê²€ì¦ ë¶ˆê°€: í‰ê°€ëœ ì‚¬ìš©ì ìˆ˜ ë¶€ì¡±")
    else:
        # Precision ë¹„êµ
        t_stat_prec, p_value_prec = stats.ttest_rel(
            results_max['precision_scores'][:min_len],
            results_avg['precision_scores'][:min_len]
        )

        # NDCG ë¹„êµ
        t_stat_ndcg, p_value_ndcg = stats.ttest_rel(
            results_max['ndcg_scores'][:min_len],
            results_avg['ndcg_scores'][:min_len]
        )

        # íš¨ê³¼ í¬ê¸° (Cohen's d)
        def cohens_d(x, y):
            nx, ny = len(x), len(y)
            dof = nx + ny - 2
            return (np.mean(x) - np.mean(y)) / np.sqrt(((nx-1)*np.std(x, ddof=1)**2 + (ny-1)*np.std(y, ddof=1)**2) / dof)

        effect_prec = cohens_d(results_max['precision_scores'][:min_len], results_avg['precision_scores'][:min_len])
        effect_ndcg = cohens_d(results_max['ndcg_scores'][:min_len], results_avg['ndcg_scores'][:min_len])

        print(f"\nPrecision@10:")
        print(f"  t-statistic: {t_stat_prec:.4f}")
        print(f"  p-value: {p_value_prec:.4f} {'âœ… ìœ ì˜í•¨' if p_value_prec < 0.05 else 'âŒ ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
        print(f"  Cohen's d: {effect_prec:.4f} ({interpret_effect_size(effect_prec)})")

        print(f"\nNDCG@10:")
        print(f"  t-statistic: {t_stat_ndcg:.4f}")
        print(f"  p-value: {p_value_ndcg:.4f} {'âœ… ìœ ì˜í•¨' if p_value_ndcg < 0.05 else 'âŒ ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
        print(f"  Cohen's d: {effect_ndcg:.4f} ({interpret_effect_size(effect_ndcg)})")

    # 7. ê²°ê³¼ ë¹„êµ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ“Š ìµœì¢… í‰ê°€ ê²°ê³¼")
    print("="*60)

    print(f"\nğŸ”µ ìµœëŒ€ ìœ ì‚¬ë„ ë°©ì‹:")
    print(f"  Precision@10: {results_max['precision@10']:.4f}")
    print(f"  Recall@10: {results_max['recall@10']:.4f}")
    print(f"  NDCG@10: {results_max['ndcg@10']:.4f}")
    print(f"  Diversity: {results_max['diversity']:.4f}")
    print(f"  í‰ê·  ì¶”ì²œ ì‹œê°„: {results_max['avg_time']:.3f}s (Â±{results_max['std_time']:.3f}s)")

    print(f"\nğŸ”´ í‰ê·  ì„ë² ë”© ë°©ì‹:")
    print(f"  Precision@10: {results_avg['precision@10']:.4f}")
    print(f"  Recall@10: {results_avg['recall@10']:.4f}")
    print(f"  NDCG@10: {results_avg['ndcg@10']:.4f}")
    print(f"  Diversity: {results_avg['diversity']:.4f}")
    print(f"  í‰ê·  ì¶”ì²œ ì‹œê°„: {results_avg['avg_time']:.3f}s (Â±{results_avg['std_time']:.3f}s)")

    print(f"\nğŸŸ¢ í‰ê·  ìœ ì‚¬ë„ ë°©ì‹:")
    print(f"  Precision@10: {results_mean['precision@10']:.4f}")
    print(f"  Recall@10: {results_mean['recall@10']:.4f}")
    print(f"  NDCG@10: {results_mean['ndcg@10']:.4f}")
    print(f"  Diversity: {results_mean['diversity']:.4f}")
    print(f"  í‰ê·  ì¶”ì²œ ì‹œê°„: {results_mean['avg_time']:.3f}s (Â±{results_mean['std_time']:.3f}s)")

    # ê°œì„ ìœ¨ ê³„ì‚° (í‰ê·  ì„ë² ë”© ê¸°ì¤€)
    if results_avg['precision@10'] > 0:
        print(f"\nğŸ“ˆ ê°œì„ ìœ¨ (í‰ê·  ì„ë² ë”© ê¸°ì¤€):")
        
        # ìµœëŒ€ ìœ ì‚¬ë„ vs í‰ê·  ì„ë² ë”©
        max_prec_improvement = (results_max['precision@10'] - results_avg['precision@10']) / results_avg['precision@10'] * 100
        max_recall_improvement = (results_max['recall@10'] - results_avg['recall@10']) / results_avg['recall@10'] * 100 if results_avg['recall@10'] > 0 else 0
        max_ndcg_improvement = (results_max['ndcg@10'] - results_avg['ndcg@10']) / results_avg['ndcg@10'] * 100
        max_div_improvement = (results_max['diversity'] - results_avg['diversity']) / results_avg['diversity'] * 100 if results_avg['diversity'] > 0 else 0
        max_time_change = (results_max['avg_time'] - results_avg['avg_time']) / results_avg['avg_time'] * 100 if results_avg['avg_time'] > 0 else 0

        # í‰ê·  ìœ ì‚¬ë„ vs í‰ê·  ì„ë² ë”©
        mean_prec_improvement = (results_mean['precision@10'] - results_avg['precision@10']) / results_avg['precision@10'] * 100
        mean_recall_improvement = (results_mean['recall@10'] - results_avg['recall@10']) / results_avg['recall@10'] * 100 if results_avg['recall@10'] > 0 else 0
        mean_ndcg_improvement = (results_mean['ndcg@10'] - results_avg['ndcg@10']) / results_avg['ndcg@10'] * 100
        mean_div_improvement = (results_mean['diversity'] - results_avg['diversity']) / results_avg['diversity'] * 100 if results_avg['diversity'] > 0 else 0
        mean_time_change = (results_mean['avg_time'] - results_avg['avg_time']) / results_avg['avg_time'] * 100 if results_avg['avg_time'] > 0 else 0

        print(f"\n  ğŸ”µ ìµœëŒ€ ìœ ì‚¬ë„ vs í‰ê·  ì„ë² ë”©:")
        print(f"    Precision@10: {max_prec_improvement:+.2f}%")
        print(f"    Recall@10: {max_recall_improvement:+.2f}%")
        print(f"    NDCG@10: {max_ndcg_improvement:+.2f}%")
        print(f"    Diversity: {max_div_improvement:+.2f}%")
        print(f"    ì¶”ì²œ ì‹œê°„: {max_time_change:+.2f}%")

        print(f"\n  ğŸŸ¢ í‰ê·  ìœ ì‚¬ë„ vs í‰ê·  ì„ë² ë”©:")
        print(f"    Precision@10: {mean_prec_improvement:+.2f}%")
        print(f"    Recall@10: {mean_recall_improvement:+.2f}%")
        print(f"    NDCG@10: {mean_ndcg_improvement:+.2f}%")
        print(f"    Diversity: {mean_div_improvement:+.2f}%")
        print(f"    ì¶”ì²œ ì‹œê°„: {mean_time_change:+.2f}%")

        # 8. ì˜ì‚¬ê²°ì • ê°€ì´ë“œ
        print(f"\n" + "="*60)
        print("ğŸ’¡ ì˜ì‚¬ê²°ì • ê°€ì´ë“œ")
        print("="*60)

        # í’ˆì§ˆ ê°œì„  ì—¬ë¶€ (í‰ê·  ì„ë² ë”© ëŒ€ë¹„)
        if min_len >= 2:
            max_quality_improved = (max_prec_improvement > 10 and max_ndcg_improvement > 10 and
                                   p_value_prec < 0.05 and p_value_ndcg < 0.05)
            mean_quality_improved = (mean_prec_improvement > 10 and mean_ndcg_improvement > 10)
        else:
            max_quality_improved = (max_prec_improvement > 10 and max_ndcg_improvement > 10)
            mean_quality_improved = (mean_prec_improvement > 10 and mean_ndcg_improvement > 10)

        # ì†ë„ í—ˆìš© ì—¬ë¶€
        max_speed_acceptable = results_max['avg_time'] < 5.0
        mean_speed_acceptable = results_mean['avg_time'] < 5.0

        # ìµœëŒ€ ìœ ì‚¬ë„ í‰ê°€
        if max_quality_improved and max_speed_acceptable:
            print("\nâœ… ìµœëŒ€ ìœ ì‚¬ë„ ë°©ì‹ ì±„íƒ ê¶Œì¥")
            print("  ì´ìœ :")
            print(f"  - Precision ê°œì„ : {max_prec_improvement:+.2f}% (ëª©í‘œ: >10%)")
            print(f"  - NDCG ê°œì„ : {max_ndcg_improvement:+.2f}% (ëª©í‘œ: >10%)")
            if min_len >= 2:
                print(f"  - í†µê³„ì  ìœ ì˜ì„±: p < 0.05")
            print(f"  - ì¶”ì²œ ì‹œê°„: {results_max['avg_time']:.2f}s (ëª©í‘œ: <5s)")
        elif max_quality_improved and not max_speed_acceptable:
            print("\nâš ï¸ ìµœëŒ€ ìœ ì‚¬ë„ ë°©ì‹ - ì¡°ê±´ë¶€ ì±„íƒ")
            print("  ì¥ì : í’ˆì§ˆ ê°œì„  ìœ ì˜ë¯¸")
            print("  ë‹¨ì : ì¶”ì²œ ì‹œê°„ ì¦ê°€")
            print("  ì œì•ˆ: ì„±ëŠ¥ ìµœì í™” í›„ ì¬í‰ê°€")

        # í‰ê·  ìœ ì‚¬ë„ í‰ê°€
        if mean_quality_improved and mean_speed_acceptable:
            print("\nâœ… í‰ê·  ìœ ì‚¬ë„ ë°©ì‹ ì±„íƒ ê¶Œì¥")
            print("  ì´ìœ :")
            print(f"  - Precision ê°œì„ : {mean_prec_improvement:+.2f}% (ëª©í‘œ: >10%)")
            print(f"  - NDCG ê°œì„ : {mean_ndcg_improvement:+.2f}% (ëª©í‘œ: >10%)")
            print(f"  - ì¶”ì²œ ì‹œê°„: {results_mean['avg_time']:.2f}s (ëª©í‘œ: <5s)")
        elif mean_quality_improved and not mean_speed_acceptable:
            print("\nâš ï¸ í‰ê·  ìœ ì‚¬ë„ ë°©ì‹ - ì¡°ê±´ë¶€ ì±„íƒ")
            print("  ì¥ì : í’ˆì§ˆ ê°œì„  ìœ ì˜ë¯¸")
            print("  ë‹¨ì : ì¶”ì²œ ì‹œê°„ ì¦ê°€")
            print("  ì œì•ˆ: ì„±ëŠ¥ ìµœì í™” í›„ ì¬í‰ê°€")

        # ë‘˜ ë‹¤ ê°œì„ ì´ ì—†ëŠ” ê²½ìš°
        if not max_quality_improved and not mean_quality_improved:
            print("\nâŒ í‰ê·  ì„ë² ë”© ë°©ì‹ ìœ ì§€ ê¶Œì¥")
            print("  ì´ìœ :")
            print("  - ìµœëŒ€/í‰ê·  ìœ ì‚¬ë„ ëª¨ë‘ í’ˆì§ˆ ê°œì„  ë¯¸ë¯¸")
            print("  - ë˜ëŠ” í†µê³„ì  ìœ ì˜ì„± ë¶€ì¡±")

    print(f"\n" + "="*60)
    print("âœ… í‰ê°€ ì™„ë£Œ")
    print("="*60)


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
